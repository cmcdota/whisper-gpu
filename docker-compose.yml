services:
  whisper-server-gpu:
    image: whisper-server-gpu
    build:
      context: .
      dockerfile: Dockerfile
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
    runtime: nvidia
    environment:
      - LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/lib64/stubs
    volumes:
      - ./models:/models
    restart: always
    ports:
      - "0.0.0.0:8080:8080"
    command: ["-m", "/models/ggml-large-v3-turbo.bin","-l", "ru",
      "-p", "2",
      "-t", "16",
      "--convert",
      "--vad",
      "--vad-model", "/models/ggml-silero-v5.1.2.bin",
      "--max-context", "512",
      "--host","0.0.0.0",
      "--port","8080"]
